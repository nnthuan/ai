# üìö Loss Function
**Loss** (hay c√≤n g·ªçi l√† **Cost Function**) l√† m·ªôt h√†m s·ªë d√πng ƒë·ªÉ ƒëo **m·ª©c ƒë·ªô sai l·ªách** gi·ªØa **gi√° tr·ªã th·ª±c t·∫ø** v√† **gi√° tr·ªã d·ª± ƒëo√°n** c·ªßa m√¥ h√¨nh.

N√≥i ƒë∆°n gi·∫£n, **Loss function** s·∫Ω tr·∫£ v·ªÅ m·ªôt gi√° tr·ªã ƒë·∫°i di·ªán cho "s·ª± kh√°c bi·ªát" gi·ªØa d·ª± ƒëo√°n c·ªßa m√¥ h√¨nh v√† th·ª±c t·∫ø. M·ª•c ti√™u c·ªßa vi·ªác hu·∫•n luy·ªán m√¥ h√¨nh l√† **gi·∫£m thi·ªÉu Loss** n√†y xu·ªëng c√†ng th·∫•p c√†ng t·ªët.

## üéØ C√°c lo·∫°i Loss Function
C√≥ nhi·ªÅu lo·∫°i Loss Function tu·ª≥ theo b√†i to√°n m√† ch√∫ng ta ƒëang gi·∫£i quy·∫øt. T√¥i s·∫Ω gi·ªõi thi·ªáu 2 lo·∫°i ch√≠nh:

### 1. Mean Squared Error (MSE) - Ph∆∞∆°ng sai trung b√¨nh
**C√¥ng th·ª©c:**

$$
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

Trong ƒë√≥:ƒê·∫°i h·ªçc S∆∞ Ph·∫°m K·ªπ Thu·∫≠t TPHCM, ƒê∆∞·ªùng V√µ VƒÉn Ng√¢n, Linh Chi·ªÉu, Th·ªß ƒê·ª©c City, Ho Chi Minh City

$\hat{y}_i$ l√† gi√° tr·ªã d·ª± ƒëo√°n t·ª´ m√¥ h√¨nh.

$y_i$ l√† gi√° tr·ªã th·ª±c t·∫ø.

$n$ l√† s·ªë l∆∞·ª£ng d·ªØ li·ªáu m·∫´u.

**C√°ch ho·∫°t ƒë·ªông:**

- MSE t√≠nh sai s·ªë b√¨nh ph∆∞∆°ng gi·ªØa d·ª± ƒëo√°n v√† th·ª±c t·∫ø, sau ƒë√≥ t√≠nh trung b√¨nh c·ªßa t·∫•t c·∫£ c√°c sai s·ªë.

- MSE c√≥ gi√° tr·ªã c√†ng nh·ªè th√¨ m√¥ h√¨nh c√†ng ch√≠nh x√°c.

**V√≠ d·ª•:**

N·∫øu m√¥ h√¨nh d·ª± ƒëo√°n gi√° nh√† l√† 250k nh∆∞ng gi√° th·ª±c t·∫ø l√† 230k, sai s·ªë l√† 20k. N·∫øu ta l·∫•y b√¨nh ph∆∞∆°ng (20k * 20k = 400k), k·∫øt qu·∫£ loss n√†y s·∫Ω cho bi·∫øt m·ª©c ƒë·ªô sai l·ªách ch√≠nh x√°c.

**T·∫°i sao l·∫°i ph·∫£i b√¨nh ph∆∞∆°ng sai s·ªë trong MSE?**

1. Lo·∫°i b·ªè d·∫•u √¢m v√† ƒë·∫£m b·∫£o t√≠nh d∆∞∆°ng

    M·ªôt trong nh·ªØng l√Ω do ch√≠nh ƒë·ªÉ **b√¨nh ph∆∞∆°ng** sai s·ªë l√† ƒë·ªÉ **lo·∫°i b·ªè d·∫•u √¢m**.

    Khi t√≠nh sai s·ªë gi·ªØa gi√° tr·ªã d·ª± ƒëo√°n $\hat{y}$ v√† gi√° tr·ªã th·ª±c t·∫ø $y$, ta c√≥ th·ªÉ g·∫∑p ph·∫£i nh·ªØng sai s·ªë √¢m v√† d∆∞∆°ng. N·∫øu kh√¥ng b√¨nh ph∆∞∆°ng sai s·ªë, ch√∫ng s·∫Ω cancel nhau khi t√≠nh trung b√¨nh, v√† l√†m cho gi√° tr·ªã Loss b·ªã l·ªách ƒëi.

    V√≠ d·ª•:
    - $\hat{y_1}=10,y_1=12$ sai s·ªë l√† $-2$.
    - $\hat{y_2}=14,y_2=13$ sai s·ªë l√† $+1$.
    
    N·∫øu kh√¥ng b√¨nh ph∆∞∆°ng, t·ªïng sai s·ªë l√† $-2+1=-1$. Nh∆∞ng khi ta b√¨nh ph∆∞∆°ng, s·∫Ω c√≥:
    - Sai s·ªë ƒë·∫ßu ti√™n l√† $(-2)^2=4$.
    - Sai s·ªë th·ª© hai l√† $(-1)^2=1$.

    T·ªïng l√† $4+1=5$ v√† t·∫•t c·∫£ sai s·ªë ƒë·ªÅu ƒë√≥ng g√≥p v√†o vi·ªác t√≠nh to√°n.

B·∫±ng c√°ch b√¨nh ph∆∞∆°ng, ta t√≠nh **t·ªïng t·∫•t c·∫£ c√°c sai s·ªë** m√† kh√¥ng b·ªã ·∫£nh h∆∞·ªüng b·ªüi d·∫•u √¢m.

2. T·∫°o s·ª± ch√∫ √Ω ƒë·∫øn c√°c sai s·ªë l·ªõn

    Khi ta b√¨nh ph∆∞∆°ng sai s·ªë, c√°c sai s·ªë l·ªõn s·∫Ω c√≥ ·∫£nh h∆∞·ªüng n·∫∑ng n·ªÅ h∆°n so v·ªõi sai s·ªë nh·ªè.

    V√≠ d·ª•:

    N·∫øu m√¥ h√¨nh d·ª± ƒëo√°n g·∫ßn ƒë√∫ng (sai s·ªë nh·ªè) th√¨ kh√¥ng sao, nh∆∞ng n·∫øu d·ª± ƒëo√°n sai qu√° nhi·ªÅu (sai s·ªë l·ªõn), ta mu·ªën m√¥ h√¨nh ph·∫£i ch√∫ √Ω h∆°n ƒë·∫øn vi·ªác gi·∫£m sai s·ªë l·ªõn.

    B√¨nh ph∆∞∆°ng s·∫Ω l√†m cho sai s·ªë l·ªõn (v√≠ d·ª•, 100 ho·∫∑c 1000) tr·ªü n√™n r·∫•t l·ªõn trong t·ªïng Loss, khi·∫øn m√¥ h√¨nh c·ªë g·∫Øng gi·∫£m c√°c sai s·ªë l·ªõn n√†y m·ªôt c√°ch m·∫°nh m·∫Ω h∆°n.

3. ƒê·∫£m b·∫£o ƒë·ªô li√™n t·ª•c v√† ƒë·∫°o h√†m d·ªÖ t√≠nh

    B√¨nh ph∆∞∆°ng sai s·ªë gi√∫p Loss c√≥ t√≠nh li√™n t·ª•c v√† ƒë·∫°o h√†m d·ªÖ d√†ng (khi ta s·ª≠ d·ª•ng Gradient Descent).

    N·∫øu kh√¥ng b√¨nh ph∆∞∆°ng sai s·ªë, Loss c√≥ th·ªÉ kh√¥ng li√™n t·ª•c v√† kh√¥ng d·ªÖ t√≠nh to√°n ƒë·∫°o h√†m.

    B√¨nh ph∆∞∆°ng gi√∫p t·∫°o ra h√†m m∆∞·ª£t m√† v√† d·ªÖ d√†ng t·ªëi ∆∞u h√≥a, v√¨ Gradient Descent s·∫Ω t√≠nh to√°n c√°c ƒë·∫°o h√†m c·ªßa Loss ƒë·ªÉ c·∫≠p nh·∫≠t tham s·ªë w, b.

4. Thu·∫≠t to√°n d·ªÖ hi·ªÉu v√† d·ªÖ tri·ªÉn khai

    Vi·ªác b√¨nh ph∆∞∆°ng sai s·ªë gi√∫p h√†m Loss function ƒë∆°n gi·∫£n h∆°n trong c√°c thu·∫≠t to√°n t·ªëi ∆∞u, gi√∫p gi·∫£m thi·ªÉu chi ph√≠ t√≠nh to√°n khi s·ª≠ d·ª•ng c√°c thu·∫≠t to√°n t·ªëi ∆∞u nh∆∞ Gradient Descent.

üéØ T√≥m l·∫°i l√Ω do t·∫°i sao b√¨nh ph∆∞∆°ng sai s·ªë trong MSE:

- **Lo·∫°i b·ªè d·∫•u √¢m**: B√¨nh ph∆∞∆°ng gi√∫p tr√°nh vi·ªác sai s·ªë √¢m v√† d∆∞∆°ng cancel nhau.

- **Ch√∫ tr·ªçng sai s·ªë l·ªõn**: Sai s·ªë l·ªõn s·∫Ω c√≥ ·∫£nh h∆∞·ªüng m·∫°nh m·∫Ω h∆°n v√† gi√∫p m√¥ h√¨nh c·∫£i thi·ªán.

- **ƒê·∫£m b·∫£o t√≠nh li√™n t·ª•c v√† ƒë·∫°o h√†m**: MSE c√≥ t√≠nh m∆∞·ª£t m√†, gi√∫p thu·∫≠t to√°n t·ªëi ∆∞u nh∆∞ Gradient Descent ho·∫°t ƒë·ªông hi·ªáu qu·∫£ h∆°n.

- **Thu·∫≠t to√°n d·ªÖ tri·ªÉn khai**: B√¨nh ph∆∞∆°ng gi√∫p gi·∫£m ƒë·ªô ph·ª©c t·∫°p c·ªßa t√≠nh to√°n.

### 2. Binary Cross-Entropy (Log Loss) ‚Äì Ph√¢n lo·∫°i nh·ªã ph√¢n
Khi b√†i to√°n c·ªßa b·∫°n l√† ph√¢n lo·∫°i (nh∆∞ ph√¢n lo·∫°i email l√† spam hay kh√¥ng), Loss Function ph·ªï bi·∫øn l√† Binary Cross-Entropy.

**C√¥ng th·ª©c:**

$$
Loss = -\frac{1}{n} \sum_{i=1}^n[y_ilog(\hat{y_i})+(1-y_i)log(1-\hat{y_i})]
$$

Trong ƒë√≥:
- $\hat{y_i}$ l√† x√°c su·∫•t d·ª± ƒëo√°n c·ªßa m√¥ h√¨nh.
- $y_i$ l√† nh√£n th·ª±c t·∫ø (0 ho·∫∑c 1).

**C√°ch ho·∫°t ƒë·ªông:**

H√†m Binary Cross-Entropy ƒë√°nh gi√° kh·∫£ nƒÉng c·ªßa m√¥ h√¨nh trong vi·ªác d·ª± ƒëo√°n x√°c su·∫•t ch√≠nh x√°c cho c√°c l·ªõp.

C√°i n√†y ƒë∆∞·ª£c d√πng ph·ªï bi·∫øn trong c√°c b√†i to√°n ph√¢n lo·∫°i nh·ªã ph√¢n (v√≠ d·ª•: spam vs kh√¥ng spam, b·ªánh vs kh√¥ng b·ªánh).

## Loss Function trong Machine Learning

**1. H∆∞·ªõng ƒëi c·ªßa m√¥ h√¨nh:**

- Trong qu√° tr√¨nh hu·∫•n luy·ªán, m√¥ h√¨nh s·∫Ω c·ªë g·∫Øng gi·∫£m Loss.

- Gradient Descent l√† c√¥ng c·ª• gi√∫p m√¥ h√¨nh t√¨m ra c√°c tham s·ªë (nh∆∞ w v√† b) sao cho Loss c√†ng nh·ªè c√†ng t·ªët.

**2. T√≠nh quan tr·ªçng c·ªßa Loss:**

- Loss th·ªÉ hi·ªán ƒë∆∞·ª£c m·ª©c ƒë·ªô hi·ªáu qu·∫£ c·ªßa m√¥ h√¨nh.

- Khi Loss nh·ªè, m√¥ h√¨nh c√†ng ch√≠nh x√°c. N·∫øu Loss l·ªõn, m√¥ h√¨nh c·∫ßn ƒë∆∞·ª£c tinh ch·ªânh th√™m (tƒÉng d·ªØ li·ªáu, thay ƒë·ªïi m√¥ h√¨nh, ƒëi·ªÅu ch·ªânh si√™u tham s·ªë...).

## K·∫øt lu·∫≠n
- **Loss** l√† th∆∞·ªõc ƒëo quan tr·ªçng cho ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh.

- **MSE** (Mean Squared Error) r·∫•t ph·ªï bi·∫øn trong b√†i to√°n **h·ªìi quy** (Regression).

- **Binary Cross-Entropy** ƒë∆∞·ª£c s·ª≠ d·ª•ng trong c√°c b√†i to√°n **ph√¢n lo·∫°i nh·ªã ph√¢n** (Classification).