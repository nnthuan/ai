# Gradient Descent và cách tìm ra w, b
## 1. Vấn đề: Tìm $w$ và $b$ sao cho Loss nhỏ nhất

- Trong bài toán Linear Regression, ta cần tìm đường thẳng tốt nhất.

- "Tốt nhất" = sai số (loss) giữa dự đoán $\hat{y}$ và thực tế $y$ là nhỏ nhất.

Loss được đo bằng công thức:

$$
Loss = \frac{1}{n}\sum(\hat{y_i}-y_i)^2 = \frac{1}{n}\sum(wx_i+b-y_i)^2
$$

→ Bạn thấy không? Loss phụ thuộc vào w và b.

## 2. Vậy làm sao tìm ra $w$ và $b$ tốt nhất?

- Ý tưởng: Đi tìm $w$, $b$ sao cho Loss nhỏ nhất.

- Bài toán này trở thành bài toán tối ưu hóa.

- Và để giải bài toán tối ưu hóa này, Gradient Descent chính là công cụ chủ lực.

## 3. Gradient Descent là gì?

- **Gradient Descent** = "Đi xuống theo hướng dốc nhất" để **giảm Loss**.

- Ta tưởng tượng Loss như 1 ngọn núi → Bạn cần **bò xuống núi** tới điểm thấp nhất (Loss = nhỏ nhất).

Minh họa đơn giản:

- Bạn đang đứng trên sườn núi (Loss cao).

- Bạn nhìn xung quanh → thấy hướng nào dốc xuống nhất → bước một bước nhỏ xuống.

- Cứ lặp lại bước đó nhiều lần → cuối cùng sẽ xuống đến chân núi (Loss cực tiểu).


## 4. Cách Gradient Descent cập nhật $w$ và $b$

**1. Tính gradient** (đạo hàm) của Loss theo $w$ và $b$.

  - Gradient chỉ ra hướng mà Loss tăng nhanh nhất.

  - Ta **đi ngược lại** hướng đó để giảm Loss.

**2. Cập nhật tham số:**

Công thức cập nhật:

$$
w := w - \alpha \frac{\partial \text{Loss}}{\partial w}
$$

$$
b := b - \alpha \frac{\partial \text{Loss}}{\partial b}
$$

Trong đó:
- $\alpha$ là **learing rate** (tốc độ số học), số nhỏ như 0.01, 0.001.
- $\frac{\partial \text{Loss}}{\partial w}$, $\frac{\partial \text{Loss}}{\partial b}$ là **đạo hàm** của Loss theo $w$ và $b$.

### Vậy cụ thể gradient là gì?

Đạo hàm Loss theo $w$:

$$
\frac{\partial \text{Loss}}{\partial w} = \frac{2}{n}\sum(wx_i+b-y_i)x_i
$$

Đạo hàm Loss theo $bw$:

$$
\frac{\partial \text{Loss}}{\partial b} = \frac{2}{n}\sum(bx_i+b-y_i)
$$

**Nhận xét:**

- Gradient cho w liên quan tới **x**.

- Gradient cho b **không liên quan tới x**.

## 5. Tóm tắt quy trình Gradient Descent cho Linear Regression

| Bước | Mô tả |
|:---|:---|
| 1 | Khởi tạo w, b ngẫu nhiên |
| 2 | Tính Loss với w, b hiện tại |
| 3 | Tính gradient (đạo hàm) của Loss theo w và b |
| 4 | Cập nhật w và b theo hướng ngược gradient |
| 5 | Lặp lại cho đến khi Loss đủ nhỏ |

### Một hình ảnh minh họa dễ hiểu:

```nginx
Loss
  ^
  |
  |           .
  |        .    bước nhỏ
  |     .          .
  |  .                .
  |--------------------------> w
```

- Điểm "." tượng trưng cho các bước cập nhật w để loss giảm dần.

- Càng gần cực tiểu, bước càng nhỏ để không bị "trượt qua".

## Kết luận:
- Gradient Descent giúp mô hình học bằng cách bò xuống núi loss.

- Qua mỗi epoch, w và b được chỉnh một chút để làm Loss nhỏ hơn.

- Sau nhiều epoch, ta có một đường thẳng dự đoán rất gần dữ liệu thật.


## Coding

1. Bắt đầu với dữ liệu mẫu

```python
import numpy as np
import matplotlib.pyplot as plt

# Dữ liệu mẫu (diện tích nhà và giá nhà)
x = np.array([50, 80, 120])
y = np.array([150, 250, 350])
```

2. Khởi tạo tham số

```python
# Khởi tạo ngẫu nhiên w và b
w = np.random.randn()
b = np.random.randn()

# Các siêu tham số
learning_rate = 0.0001
epochs = 1000
```

3. Gradient Descent "chân phương"

```python
# Huấn luyện mô hình
for epoch in range(epochs):
    # 1. Dự đoán
    y_pred = w * x + b
    
    # 2. Tính Loss
    loss = np.mean((y_pred - y) ** 2)
    
    # 3. Tính gradient
    dw = np.mean(2 * (y_pred - y) * x)
    db = np.mean(2 * (y_pred - y))
    
    # 4. Cập nhật tham số
    w -= learning_rate * dw
    b -= learning_rate * db
    
    # 5. In loss mỗi 100 epochs
    if epoch % 100 == 0:
        print(f"Epoch {epoch}: Loss = {loss:.4f}, w = {w:.4f}, b = {b:.4f}")
```

4. Sau huấn luyện, vẽ kết quả

```python
# Vẽ dữ liệu thật và đường thẳng dự đoán
plt.scatter(x, y, color='blue', label='Dữ liệu thật')
plt.plot(x, w * x + b, color='red', label='Đường dự đoán')
plt.xlabel('Diện tích nhà (m²)')
plt.ylabel('Giá nhà (nghìn USD)')
plt.legend()
plt.show()
```

**Giải thích nhanh:**

| Bước | Ý nghĩa |
|:---|:---|
| y_pred | Tính giá trị dự đoán dựa trên w và b hiện tại |
| loss | Tính sai số trung bình giữa dự đoán và thực tế |
| dw, db | Tính đạo hàm để biết hướng điều chỉnh w, b |
| cập nhật w, b | Dịch w và b theo hướng giảm loss |

**Kết quả mong đợi:**

- Ban đầu loss khá lớn.

- Sau nhiều vòng (epoch), loss sẽ giảm dần.

- Cuối cùng, đường thẳng (đường màu đỏ) sẽ fit sát dữ liệu thật (các điểm xanh).