# üìö B√†i h·ªçc 3: Th·ª±c h√†nh ‚Äî X√¢y d·ª±ng Linear Regression t·ª´ ƒë·∫ßu
## 1. B√†i to√°n ƒë∆°n gi·∫£n:
Gi·∫£ s·ª≠ ta c√≥ t·∫≠p d·ªØ li·ªáu:

|Di·ªán t√≠ch nh√† (m¬≤)| Gi√° nh√† (ngh√¨n USD)|
|------------------|--------------------|
|50 | 150|
|80 | 250|
|120 | 350|

Ta mu·ªën t√¨m ra ƒë∆∞·ªùng th·∫≥ng d·ª± ƒëo√°n gi√° nh√† theo di·ªán t√≠ch.

‚Üí C√¥ng th·ª©c to√°n h·ªçc Linear Regression:

$$
\hat{y} = wx + b
$$

Trong ƒë√≥:

- $\hat{y}$ l√† gi√° d·ª± ƒëo√°n,

- $x$ l√† di·ªán t√≠ch,

- $w$ l√† h·ªá s·ªë g√≥c (slope),

- $b$ l√† h·ªá s·ªë ch·ªách (bias).

## 2. C√°c b∆∞·ªõc th·ª±c hi·ªán:
### B∆∞·ªõc 1: Import th∆∞ vi·ªán c·∫ßn thi·∫øt

```python
import numpy as np
import matplotlib.pyplot as plt
```

### B∆∞·ªõc 2: Chu·∫©n b·ªã d·ªØ li·ªáu

```python
# D·ªØ li·ªáu di·ªán t√≠ch (x) v√† gi√° nh√† (y)
x = np.array([50, 80, 120])
y = np.array([150, 250, 350])
```

### B∆∞·ªõc 3: Kh·ªüi t·∫°o tham s·ªë ban ƒë·∫ßu

```python
# Kh·ªüi t·∫°o w v√† b ng·∫´u nhi√™n
w = np.random.randn()
b = np.random.randn()

# T·ªëc ƒë·ªô h·ªçc
learning_rate = 0.0001

# S·ªë l·∫ßn l·∫∑p
epochs = 1000
```

### B∆∞·ªõc 4: Hu·∫•n luy·ªán m√¥ h√¨nh b·∫±ng Gradient Descent

```python
# Qu√° tr√¨nh hu·∫•n luy·ªán
for epoch in range(epochs):
    # D·ª± ƒëo√°n
    y_pred = w * x + b
    
    # T√≠nh loss (Mean Squared Error)
    loss = np.mean((y_pred - y) ** 2)
    
    # T√≠nh gradient
    dw = np.mean(2 * (y_pred - y) * x)
    db = np.mean(2 * (y_pred - y))
    
    # C·∫≠p nh·∫≠t tham s·ªë
    w -= learning_rate * dw
    b -= learning_rate * db
    
    # In loss m·ªói 100 epochs
    if epoch % 100 == 0:
        print(f"Epoch {epoch}: Loss = {loss:.4f}")
```

### B∆∞·ªõc 5: V·∫Ω k·∫øt qu·∫£

```python
# V·∫Ω d·ªØ li·ªáu v√† ƒë∆∞·ªùng th·∫≥ng d·ª± ƒëo√°n
plt.scatter(x, y, color='blue', label='D·ªØ li·ªáu th·∫≠t')
plt.plot(x, w * x + b, color='red', label='D·ª± ƒëo√°n')
plt.xlabel('Di·ªán t√≠ch nh√† (m¬≤)')
plt.ylabel('Gi√° nh√† (ngh√¨n USD)')
plt.legend()
plt.show()
```

### Gi·∫£i th√≠ch nhanh:
M·ªói l·∫ßn l·∫∑p (epoch), m√¥ h√¨nh t√≠nh loss ƒë·ªÉ ƒëo m·ª©c sai l·ªách.

Sau ƒë√≥ m√¥ h√¨nh t√≠nh gradient v√† c·∫≠p nh·∫≠t w, b ƒë·ªÉ gi·∫£m loss.

Sau nhi·ªÅu l·∫ßn l·∫∑p, ƒë∆∞·ªùng th·∫≥ng s·∫Ω fit d·ªØ li·ªáu t·ªët h∆°n.